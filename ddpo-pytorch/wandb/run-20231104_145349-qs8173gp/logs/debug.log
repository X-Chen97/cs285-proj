2023-11-04 14:53:49,642 INFO    MainThread:49187 [wandb_setup.py:_flush():76] Current SDK version is 0.15.12
2023-11-04 14:53:49,643 INFO    MainThread:49187 [wandb_setup.py:_flush():76] Configure stats pid to 49187
2023-11-04 14:53:49,643 INFO    MainThread:49187 [wandb_setup.py:_flush():76] Loading settings from /global/home/users/chenxin0210/.config/wandb/settings
2023-11-04 14:53:49,643 INFO    MainThread:49187 [wandb_setup.py:_flush():76] Loading settings from /global/scratch/users/chenxin0210/cs285-proj/ddpo-pytorch/wandb/settings
2023-11-04 14:53:49,643 INFO    MainThread:49187 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2023-11-04 14:53:49,643 INFO    MainThread:49187 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2023-11-04 14:53:49,643 INFO    MainThread:49187 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'ddpo-pytorch/scripts/train.py', 'program_abspath': '/global/scratch/users/chenxin0210/cs285-proj/ddpo-pytorch/scripts/train.py', 'program': '/global/scratch/users/chenxin0210/cs285-proj/ddpo-pytorch/scripts/train.py'}
2023-11-04 14:53:49,643 INFO    MainThread:49187 [wandb_init.py:_log_setup():528] Logging user logs to /global/scratch/users/chenxin0210/cs285-proj/ddpo-pytorch/wandb/run-20231104_145349-qs8173gp/logs/debug.log
2023-11-04 14:53:49,644 INFO    MainThread:49187 [wandb_init.py:_log_setup():529] Logging internal logs to /global/scratch/users/chenxin0210/cs285-proj/ddpo-pytorch/wandb/run-20231104_145349-qs8173gp/logs/debug-internal.log
2023-11-04 14:53:49,644 INFO    MainThread:49187 [wandb_init.py:init():568] calling init triggers
2023-11-04 14:53:49,644 INFO    MainThread:49187 [wandb_init.py:init():575] wandb.init called with sweep_config: {}
config: {}
2023-11-04 14:53:49,644 INFO    MainThread:49187 [wandb_init.py:init():618] starting backend
2023-11-04 14:53:49,644 INFO    MainThread:49187 [wandb_init.py:init():622] setting up manager
2023-11-04 14:53:49,647 INFO    MainThread:49187 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2023-11-04 14:53:49,652 INFO    MainThread:49187 [wandb_init.py:init():628] backend started and connected
2023-11-04 14:53:49,656 INFO    MainThread:49187 [wandb_init.py:init():720] updated telemetry
2023-11-04 14:53:49,683 INFO    MainThread:49187 [wandb_init.py:init():753] communicating run to backend with 90.0 second timeout
2023-11-04 14:53:49,995 INFO    MainThread:49187 [wandb_run.py:_on_init():2220] communicating current version
2023-11-04 14:53:50,097 INFO    MainThread:49187 [wandb_run.py:_on_init():2229] got version response 
2023-11-04 14:53:50,098 INFO    MainThread:49187 [wandb_init.py:init():804] starting run threads in backend
2023-11-04 14:54:02,552 INFO    MainThread:49187 [wandb_run.py:_console_start():2199] atexit reg
2023-11-04 14:54:02,552 INFO    MainThread:49187 [wandb_run.py:_redirect():2054] redirect: wrap_raw
2023-11-04 14:54:02,553 INFO    MainThread:49187 [wandb_run.py:_redirect():2119] Wrapping output streams.
2023-11-04 14:54:02,553 INFO    MainThread:49187 [wandb_run.py:_redirect():2144] Redirects installed.
2023-11-04 14:54:02,555 INFO    MainThread:49187 [wandb_init.py:init():845] run started, returning control to user process
2023-11-04 14:54:02,555 INFO    MainThread:49187 [wandb_run.py:_config_callback():1324] config_cb None None {'allow_tf32': True, 'logdir': 'logs', 'mixed_precision': 'fp16', 'num_checkpoint_limit': 100000000, 'num_epochs': 1, 'per_prompt_stat_tracking': {'buffer_size': 16, 'min_count': 16}, 'pretrained': {'model': 'CompVis/stable-diffusion-v1-4', 'revision': 'main'}, 'prompt_fn': 'engineers', 'prompt_fn_kwargs': {}, 'resume_from': '', 'reward_fn': 'gender_equality_score', 'run_name': '2023.11.04_14.53.32', 'sample': {'batch_size': 4, 'eta': 1.0, 'guidance_scale': 5.0, 'num_batches_per_epoch': 4, 'num_steps': 50}, 'save_freq': 1, 'seed': 42, 'train': {'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'adam_weight_decay': 0.0001, 'adv_clip_max': 5, 'batch_size': 2, 'cfg': True, 'clip_range': 0.0001, 'gradient_accumulation_steps': 2, 'learning_rate': 0.0003, 'max_grad_norm': 1.0, 'num_inner_epochs': 1, 'timestep_fraction': 1.0, 'use_8bit_adam': False}, 'use_lora': True}
